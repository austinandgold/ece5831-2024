{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(60000,)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('uint8')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1571c5232b0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x_test[500], cmap=plt.cm.gray)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Add two layer net in Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(100, activation=\"sigmoid\", input_shape=(784, )),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"SGD\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.utils.to_categorical(y_train, num_classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.reshape(x_train.shape[0], x_train.shape[1]*x_train.shape[2])\n",
    "x_test = x_test.reshape(x_test.shape[0], x_test.shape[1]*x_test.shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train/255.0\n",
    "x_test = x_test/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = keras.utils.to_categorical(y_train, num_classes=10)\n",
    "y_test =  keras.utils.to_categorical(y_test, num_classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_train, y_train, epochs=20, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(x_test[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = np.argmax(predictions, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.argmax(y_test[0:10], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions == labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#LeNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, AveragePooling2D, Flatten, Dense\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet:\n",
    "    def __init__(self, batch_size=32, epochs=20):\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "        self.model = None\n",
    "        self._create_lenet()\n",
    "        self._compile()\n",
    "    \n",
    "\n",
    "    def _create_lenet(self):\n",
    "        self.model = Sequential([\n",
    "            Conv2D(filters=6, kernel_size=(5,5), \n",
    "                   activation='sigmoid', input_shape=(28, 28, 1), \n",
    "                   padding='same'),\n",
    "            AveragePooling2D(pool_size=(2, 2), strides=2),\n",
    "            \n",
    "            Conv2D(filters=16, kernel_size=(5,5), \n",
    "                   activation='sigmoid', \n",
    "                   padding='same'),\n",
    "            AveragePooling2D(pool_size=(2, 2), strides=2),\n",
    "\n",
    "            Flatten(),\n",
    "\n",
    "            Dense(120, activation='sigmoid'),\n",
    "            Dense(84, activation='sigmoid'),\n",
    "            Dense(10, activation='softmax')\n",
    "        ])\n",
    "\n",
    "    def _compile(self):\n",
    "        if self.model is None:\n",
    "            print('Error: Create a model first..')\n",
    "        \n",
    "        self.model.compile(optimizer='Adam',\n",
    "                           loss='categorical_crossentropy',\n",
    "                           metrics=['accuracy'])\n",
    "        \n",
    "\n",
    "    def _preprocess(self):\n",
    "        # load mnist data\n",
    "        (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "        # normalize\n",
    "        x_train = x_train/255.0\n",
    "        x_test = x_test/255.0\n",
    "\n",
    "        # add channel dim\n",
    "        self.x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)  \n",
    "        self.x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)  \n",
    "\n",
    "        # one-hot encoding\n",
    "        self.y_train = to_categorical(y_train, 10)\n",
    "        self.y_test = to_categorical(y_test, 10)\n",
    "\n",
    "    def train(self):\n",
    "        self._preprocess()\n",
    "        self.model.fit(self.x_train, self.y_train, \n",
    "                  batch_size=self.batch_size, \n",
    "                  epochs=self.epochs)\n",
    "\n",
    "    def save(self, model_path_name):\n",
    "     \n",
    "        try:\n",
    "            # Ensure the directory exists\n",
    "            directory = os.path.dirname(model_path_name)\n",
    "            if directory and not os.path.exists(directory):\n",
    "                os.makedirs(directory)\n",
    "                \n",
    "            # Save the model as a .keras file\n",
    "            self.model.save(model_path_name)\n",
    "            print(f\"Model saved to {model_path_name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving model: {e}\")\n",
    "    \n",
    "    def load(self, model_path_name):\n",
    "    \n",
    "        try:\n",
    "            # Load the model from the .keras file\n",
    "            self.model = tensorflow.keras.models.load_model(model_path_name)\n",
    "            print(f\"Model loaded from {model_path_name}\")\n",
    "            return self.model\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model: {e}\")\n",
    "            return None\n",
    "\n",
    "    def predict(self, images):\n",
    "            \"\"\"\n",
    "            Predict the class labels for the input images.\n",
    "\n",
    "            Args:\n",
    "                images (list or np.array): A list or array of image data.\n",
    "\n",
    "            Returns:\n",
    "                list: Predicted class probabilities or class labels.\n",
    "            \"\"\"\n",
    "            try:\n",
    "                # If images is a list, convert it to a numpy array\n",
    "                if isinstance(images, list):\n",
    "                    images = np.array(images)\n",
    "                \n",
    "                # Ensure the images are in the correct format (e.g., resized, normalized)\n",
    "                # If images are raw (e.g., not preprocessed), preprocess them\n",
    "                \n",
    "                # Preprocessing images (e.g., resizing to match input shape of model)\n",
    "                processed_images = []\n",
    "                for img in images:\n",
    "                    # If the image is a PIL image or a numpy array, resize it to the expected input size\n",
    "                    # Assuming the model was trained on images of size (32, 32, 3)\n",
    "                    if isinstance(img, np.ndarray):\n",
    "                        img = image.array_to_img(img)  # Convert numpy array to a PIL image if needed\n",
    "                    \n",
    "                    # Resize the image to match model's input size (e.g., 32x32)\n",
    "                    img = img.resize((28, 28))  # Change the size if your model needs a different size\n",
    "                    img = image.img_to_array(img)  # Convert PIL image back to a numpy array\n",
    "                    \n",
    "                    # Normalize the image (if needed, e.g., values between 0 and 1)\n",
    "                    img = img / 255.0  # Assuming the model was trained with normalized images\n",
    "\n",
    "                    # Append to the list of processed images\n",
    "                    processed_images.append(img)\n",
    "\n",
    "                # Convert list of images into a numpy array for prediction\n",
    "                processed_images = np.array(processed_images)\n",
    "\n",
    "                # Make predictions with the model\n",
    "                predictions = self.model.predict(processed_images)\n",
    "                \n",
    "                # If model outputs class probabilities, you can get the predicted class labels:\n",
    "                predicted_labels = np.argmax(predictions, axis=-1)\n",
    "                \n",
    "                return predicted_labels  # or return predictions if you need probabilities\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error making predictions: {e}\")\n",
    "                return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lenet = LeNet(batch_size=64, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "938/938 [==============================] - 61s 65ms/step - loss: 1.0422 - accuracy: 0.6547\n",
      "Epoch 2/10\n",
      "938/938 [==============================] - 27s 29ms/step - loss: 0.2576 - accuracy: 0.9212\n",
      "Epoch 3/10\n",
      "938/938 [==============================] - 61s 65ms/step - loss: 0.1714 - accuracy: 0.9479\n",
      "Epoch 4/10\n",
      "938/938 [==============================] - 49s 53ms/step - loss: 0.1274 - accuracy: 0.9600\n",
      "Epoch 5/10\n",
      "938/938 [==============================] - 35s 38ms/step - loss: 0.1034 - accuracy: 0.9682\n",
      "Epoch 6/10\n",
      "938/938 [==============================] - 41s 44ms/step - loss: 0.0864 - accuracy: 0.9736\n",
      "Epoch 7/10\n",
      "938/938 [==============================] - 44s 47ms/step - loss: 0.0736 - accuracy: 0.9770\n",
      "Epoch 8/10\n",
      "938/938 [==============================] - 48s 51ms/step - loss: 0.0634 - accuracy: 0.9801\n",
      "Epoch 9/10\n",
      "938/938 [==============================] - 21s 23ms/step - loss: 0.0574 - accuracy: 0.9818\n",
      "Epoch 10/10\n",
      "938/938 [==============================] - 55s 59ms/step - loss: 0.0522 - accuracy: 0.9834\n"
     ]
    }
   ],
   "source": [
    "lenet.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m x_test \u001b[38;5;241m=\u001b[39m \u001b[43mx_test\u001b[49m\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m28\u001b[39m,\u001b[38;5;241m28\u001b[39m,\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'x_test' is not defined"
     ]
    }
   ],
   "source": [
    "x_test = x_test.reshape(-1,28,28,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = np.argmax(lenet.model.predict(x_test[0:10]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.argmax(lenet.y_test[0:10], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predictions == labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_saver = LeNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to Moore_cnn_model.keras\n"
     ]
    }
   ],
   "source": [
    "model_saver.save('Moore_cnn_model.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_loader = LeNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from Moore_cnn_model.keras\n"
     ]
    }
   ],
   "source": [
    "model_load = model_loader.load('Moore_cnn_model.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: 0_0.png\n",
      "Processing file: 0_1.png\n",
      "Processing file: 0_2.png\n",
      "Processing file: 0_3.png\n",
      "Processing file: 0_4.png\n",
      "Processing file: 1_0.png\n",
      "Processing file: 1_1.png\n",
      "Processing file: 1_2.png\n",
      "Processing file: 1_3.png\n",
      "Processing file: 1_4.png\n",
      "Processing file: 2_0.png\n",
      "Processing file: 2_1.png\n",
      "Processing file: 2_2.png\n",
      "Processing file: 2_3.png\n",
      "Processing file: 2_4.png\n",
      "Processing file: 3_0.png\n",
      "Processing file: 3_1.png\n",
      "Processing file: 3_2.png\n",
      "Processing file: 3_3.png\n",
      "Processing file: 3_4.png\n",
      "Processing file: 4_0.png\n",
      "Processing file: 4_1.png\n",
      "Processing file: 4_2.png\n",
      "Processing file: 4_3.png\n",
      "Processing file: 4_4.png\n",
      "Processing file: 5_0.png\n",
      "Processing file: 5_1.png\n",
      "Processing file: 5_2.png\n",
      "Processing file: 5_3.png\n",
      "Processing file: 5_4.png\n",
      "Processing file: 6_0.png\n",
      "Processing file: 6_1.png\n",
      "Processing file: 6_2.png\n",
      "Processing file: 6_3.png\n",
      "Processing file: 6_4.png\n",
      "Processing file: 7_0.png\n",
      "Processing file: 7_1.png\n",
      "Processing file: 7_2.png\n",
      "Processing file: 7_3.png\n",
      "Processing file: 7_4.png\n",
      "Processing file: 8_0.png\n",
      "Processing file: 8_1.png\n",
      "Processing file: 8_2.png\n",
      "Processing file: 8_3.png\n",
      "Processing file: 8_4.png\n",
      "Processing file: 9_0.png\n",
      "Processing file: 9_1.png\n",
      "Processing file: 9_2.png\n",
      "Processing file: 9_3.png\n",
      "Processing file: 9_4.png\n",
      "Processed 50 images.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "def increase_contrast(image):\n",
    "\n",
    "    # Find the minimum and maximum pixel values in the image\n",
    "    min_pixel = np.min(image)\n",
    "    max_pixel = np.max(image)\n",
    "    \n",
    "    # Apply contrast stretching (stretching pixel values to full [0, 255] range)\n",
    "    contrast_image = (image - min_pixel) * (255.0 / (max_pixel - min_pixel))\n",
    "    \n",
    "    # Clip values to ensure they remain within the valid range [0, 255]\n",
    "    contrast_image = np.clip(contrast_image, 0, 255)\n",
    "    \n",
    "    return contrast_image\n",
    "\n",
    "def load_images_from_subfolders(image_folder, target_size=(28, 28)):\n",
    "    # List to hold all image arrays\n",
    "    image_arrays = []\n",
    "\n",
    "    # Loop through the folders and files recursively\n",
    "    for subdir, _, files in os.walk(image_folder):\n",
    "        for filename in files:\n",
    "            # Only process files that end with '.PNG' (case-sensitive)\n",
    "            if filename.endswith('.png'):\n",
    "                print(f\"Processing file: {filename}\")  # Debugging line\n",
    "                # Create the full path to the image file\n",
    "                image_path = os.path.join(subdir, filename)\n",
    "\n",
    "                try:\n",
    "                    # Open the image using PIL and convert it to grayscale ('L' mode)\n",
    "                    image = Image.open(image_path).convert('L')  # Convert to grayscale\n",
    "\n",
    "                    # Resize the image to the target size (e.g., 28x28)\n",
    "                    image = image.resize(target_size)\n",
    "\n",
    "                    # Convert the resized image to a numpy array\n",
    "                    image = np.array(image)\n",
    "\n",
    "                    # Invert the image colors (255 - pixel value)\n",
    "                    image = 255.0 - image\n",
    "\n",
    "                    # Increase the contrast of the image\n",
    "                    image = increase_contrast(image)\n",
    "\n",
    "                    # Normalize the image to range [0, 255]\n",
    "                    image = (image - np.min(image)) * (255 / (np.max(image) - np.min(image)))\n",
    "\n",
    "                    # Convert to float32 and scale the values to [0, 1]\n",
    "                    image = image.astype(np.float32) / 255.0\n",
    "\n",
    "                    # Flatten the image\n",
    "                    image = image.flatten()\n",
    "\n",
    "                    # Append the processed image to the list\n",
    "                    image_arrays.append(image)\n",
    "                except Exception as e:\n",
    "                    print(f\"Failed to process {filename}: {e}\")  # Print any error\n",
    "\n",
    "    # Convert the list of image arrays into a single NumPy array (shape: num_images, flattened_pixels)\n",
    "    if len(image_arrays) == 0:\n",
    "        print(\"No images were processed.\")\n",
    "    else:\n",
    "        print(f\"Processed {len(image_arrays)} images.\")\n",
    "\n",
    "    images_numpy = np.array(image_arrays)\n",
    "\n",
    "    return images_numpy\n",
    "\n",
    "def display_image(image_numpy, index=0):\n",
    "\n",
    "    # Check if the image_numpy array has any images\n",
    "    if image_numpy.size == 0:\n",
    "        print(\"No images to display.\")\n",
    "        return\n",
    "\n",
    "    # Select an image by index from the numpy array\n",
    "    img_to_display = image_numpy[index].reshape(28, 28)  # Grayscale images have shape (28, 28)\n",
    "\n",
    "    # Display the image using matplotlib\n",
    "    plt.imshow(img_to_display, cmap='gray')  # Use 'gray' colormap for grayscale images\n",
    "    plt.axis('off')  # Hide the axes for better presentation\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "image_folder = 'Custom MNIST Sample'\n",
    "images = load_images_from_subfolders(image_folder)\n",
    "\n",
    "\n",
    "#display_image(images, index=0)\n",
    "\n",
    "# Access the image_numpy array directly\n",
    "#print(\"Shape of image_numpy:\", images.shape)  # Prints the shape of the image_numpy array\n",
    "\n",
    "# Access the first image (flattened) directly:\n",
    "#first_image = images[0]\n",
    "#print(\"First image (flattened):\", first_image)\n",
    "\n",
    "# Access the first image in original shape (28x28)\n",
    "#first_image_reshaped = first_image.reshape(28, 28)\n",
    "#print(\"First image reshaped (28x28):\\n\", first_image_reshaped)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = images.reshape(-1,28,28,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 5ms/step\n"
     ]
    }
   ],
   "source": [
    "predictions = np.argmax(lenet.model.predict(images[0:50]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 3 8 8 0 5 5 5 3 3 2 2 2 5 2 3 3 3 3 3 4 8 4 4 4 5 5 3 5 5 5 6 8 5 6 3 3\n",
      " 3 8 3 3 8 5 5 8 8 3 8 3 8]\n"
     ]
    }
   ],
   "source": [
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.10.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ece5831-2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
